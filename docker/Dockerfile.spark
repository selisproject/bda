FROM selis-hadoop-image:latest

# Specify hadoop resources.
ARG HADOOP_PREFIX=/usr/local/hadoop

# Specify spark resources.
ARG SPARK_VERSION=2.3.2
ARG SPARK_PREFIX=/usr/local/spark
ARG SPARK_URL=https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop2.7.tgz

# Install general packages.
RUN apt-get update && apt-get install --yes --no-install-recommends python3 python3-pip python3-setuptools && \
    apt-get clean autoclean && \
    pip3 install psycopg2-binary py-dateutil requests numpy pandas

# Create spark directory.
RUN mkdir -p ${SPARK_PREFIX}

# Download, install spark.
RUN curl -L ${SPARK_URL} | tar -xz --strip=1 -C ${SPARK_PREFIX}

# Add configuration files.
ADD ./bootstrap/spark/core-site.xml "${HADOOP_PREFIX}/etc/hadoop/"
ADD ./bootstrap/spark/yarn-site.xml "${HADOOP_PREFIX}/etc/hadoop/"
ADD ./bootstrap/spark/hbase-site.xml "${SPARK_PREFIX}/conf/"

# Set yarn environment variables.
ENV HADOOP_YARN_HOME   "${HADOOP_PREFIX}"
ENV YARN_CONF_DIR      "${HADOOP_PREFIX}/etc/hadoop"

# Set spark environment variables.
ENV SPARK_HOME "${SPARK_PREFIX}"
ENV PYSPARK_PYTHON "/usr/bin/python3"

ADD ./bootstrap/spark/docker-entrypoint.sh /

RUN chmod +x /docker-entrypoint.sh
